# TODO AS OF MARCH 28:
- Benchmark comparison between models: Win rate over random players (100 games) after 3 update of the new model + 1000 game after 15 update (JT) - DONE
- NN network training progression (plotting, loss metric, etc.). (JT) - DONE
- Saving of model configurations (NN and alpha zero), GPU, game played, time of training, etc. (JT) - DONE
- Baseline fine-tuning: try different HP and NN architecture, try different alpha zero parameters (# MCTS,
# batch size, etc.). If we could store some runs to compare with a baseline, it would be perfect. Like: NN changes
increases win rate by 2%, Alpha zero changes increases win rate by 3% (total tuning results of 5%)
- State representation changes: We could add features to the state representation in a similar way that we did with
getNNForm. This will add layers to the 3d representation. Very explanatory results here. (FM)
- Multiple action coding: This needs to adjust the whole code structure so that the loss gets backpropagate through
multiple networks and that MCTS gets called between different action.
- Multiple action fine-tuning: Similar to the tuning of the baseline, we can adjust the different parameters and state
representation.
- Player placement at the begining of the game
- Add 2 pawns players to the game
